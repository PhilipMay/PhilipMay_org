# LLM

## Base Knowledge

- [HF: The Alignment Handbook](https://github.com/huggingface/alignment-handbook)

## Specific Techniques

- Direct Preference Optimization (DPO)
  - Paper: <https://arxiv.org/abs/2305.18290>
  - <https://plainenglish.io/community/direct-preference-optimization-dpo-a-simplified-approach-to-fine-tuning-large-language-models>
  - <https://huggingface.co/blog/dpo-trl>

## Specific Models

- Argilla Notux
  - based on Mixtral
  - HF Model: <https://huggingface.co/argilla/notux-8x7b-v1>
  - Dataset: <https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned>
  - Code: <https://github.com/argilla-io/notus>
